{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with data using Python in a Jupyter Notebook\n",
    "You can use the Python programming language as part of your workflow to move, inspect, clean and transform data!\n",
    "\n",
    "We'll do this with the help of a few modules (also called \"libraries\" or \"packages\") in Python. \n",
    "\n",
    "## Setting up the environment\n",
    "You will first need to install modules on your computer in order for your Python code to be able to import and use them. You can use [`conda`](https://docs.conda.io/en/latest/miniconda.html) or [`pip`](https://pip.pypa.io/en/stable/) to help you install and manage these on your machine. If you're already using Jupyter Notebooks, it's likely you're already using `conda`.\n",
    "\n",
    "If using `conda` use the terminal or an Anaconda Prompt:\n",
    "\n",
    "`conda env create -f environment.yml`\n",
    "\n",
    "Activate the new environment: \n",
    "\n",
    "`conda activate crime`\n",
    "\n",
    "\n",
    "## Overview\n",
    "We'll also focus on working with dataframes as our data structure, because you'll find tables containing data all over the place and dataframes make working with tabular data easier.\n",
    "\n",
    "Using Python we can grab and mess with data from:\n",
    "\n",
    "- Sheets within Excel files;\n",
    "- Comma-seperated value (CSV) files;\n",
    "- Sheets in online Google Sheets;\n",
    "- Tables in a relational database living on a server;\n",
    "- Tables within PDF files (*ew*).\n",
    "\n",
    "You will often encounter messy tables and need to tidy them up to to enable things like doing statistics, making dashboards, and using the data in other tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding data\n",
    "Let's go over working with hardest example, tables within PDF files. Why? Because this will (hopefully) convince you never to share data in a PDF file. Also, it will take all the same tools and understanding of tidy data that will make cleaning and transforming data from spreadsheets, CSVs, and databases easier.\n",
    "\n",
    "### Crime and Police\n",
    "I've decided to use monthly crime report data that the St. Louis Metropolitan Police Department release on their [website](https://www.slmpd.org/crime_stats.shtml). They seem to have trouble with sharing this information in a useful way, so there is an opportunity to make these data into something that allows people to see what's going on in their neighborhoods.\n",
    "\n",
    "![](images/pd-website.png)\n",
    "\n",
    "Whoa... The National Incident Based Reporting System (NIBRS Crime Statistics) are released by the St. Louis Metropolitan Police Department as PDF files?!\n",
    "\n",
    "Here's the \"NIBRS Crime Comparison by Neighborhood\" file for \"2021 Jul-Aug (PDF)\"\n",
    "\n",
    "![](images/pd-crime-pdf.png)\n",
    "\n",
    "The data are \"stuck\" in these files... How are people supposed to do things like:\n",
    "\n",
    "**Extract statistics about a particular type of crime to compare across neighborhoods?**  \n",
    "*That's 178 pages to scroll through and manual copy-paste.*\n",
    "\n",
    "**Look at changes over time and compare all months side-by-side?**  \n",
    "*Each month is in a different PDF file, we need to merge data across files!*\n",
    "\n",
    "**Visualize neighborhoods and their statistics on a map?**  \n",
    "*We need to extract all these data before we can combine with geospatial data about neighborhoods*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "I've downloaded the \"NIBRS Crime Comparison by Neighborhood\" file for \"2021 Jul-Aug (PDF)\" from the [website](https://www.slmpd.org/crime_stats.shtml). The file is named \"NIBRS001-N_210708.pdf\". It gets saved to the `Downloads` folder on my computer, and I move it into the same folder as this Jupyter Notebook to make things easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages\n",
    "First, I'll import some Python modules that I know I'll use to help me work with the data. As I work, I may realize I need new modules and can add them and rerun the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the `pandas` module so that we can use its functions and objects, especially dataframes.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the `tabula` module to convert tables within PDF to dataframes or CSV files.\n",
    "import tabula\n",
    "\n",
    "# Import the `geopandas` module to work with geospatial data. \n",
    "import geopandas as gpd\n",
    "\n",
    "# Import the matplotlib` module to make plots and get a visual.\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataframe directly from a PDF file\n",
    "We'll use the [`tabula`](https://tabula-py.readthedocs.io/en/latest/) module to create a dataframe object from a table trapped in a PDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframes containing the tables that `tabula` extracts from the PDF.\n",
    "aug_pdf_file = \"crime_data/NIBRS001-N_210708.pdf\"\n",
    "\n",
    "dfs = tabula.read_pdf(aug_pdf_file, encoding='utf-8', pages='1')\n",
    "\n",
    "# Look at the dataframe we created within this Jupyter Notebook.\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Give`tabula` more arguments to try extracting from the PDF again.\n",
    "dfs = tabula.read_pdf(aug_pdf_file, encoding='utf-8', \n",
    "                     pages='1', area=(78.565,19.125,760.945,580.635), \n",
    "                     stream=True)\n",
    "\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the first dataframe in a series.\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Being lazy\n",
    "Extracting a data from a table within a PDF file into a dataframe can be fiddly, because the table formatting in the PDF file is largely arbitrary. Meaning, it may take a while to figure out exactly which part of a page `tabula` needs to look at to find a table, and to specify how it should process the individual pages of a PDF file. \n",
    "\n",
    "`tabula` does magic to make getting data out of PDF way easier, but it doesn't always work perfectly. And, these data will still be messy and need a lot of cleaning once we get them into a dataframe! \n",
    "\n",
    "If extracting directly from a PDF into dataframe is too troublesome to figure out with Python code, let the [Tabula GUI App](https://tabula.technology) help you get a CSV file that you can process with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading tabular data from a flat file into a dataframe\n",
    "\n",
    "Take a tabular file type (CSV, TSV, XLS, XLSX) and put it into a dataframe object to work with. And usually this is a file format you can download or export from things like Google Sheets or relational databases. \n",
    "\n",
    "### A tangent on static files\n",
    "The downside of working with a static file is that you won't automatically be able to pull in updates to the data that may happen elsewhere. You'll need to create a process to get a new file with the new data for updates. This could be troublesome for some data, or it could be exactly what you want. \n",
    "\n",
    "Other python modules can help you with pulling data from the source, a few examples below. \n",
    "\n",
    "**Database connections**\n",
    "you can use an adapter to connect to a database within your code:\n",
    "\n",
    "- `sqlite3` for [SQLite databases](https://docs.python.org/3/library/sqlite3.html)\n",
    "- `psycopg2` for [PostgreSQL databases](https://www.psycopg.org/docs/usage.html)\n",
    "- `mysql-connector-python` for [MySQL databases](https://www.w3schools.com/python/python_mysql_getstarted.asp)\n",
    "\n",
    "**Google Sheets API connections**\n",
    "\n",
    "- [Google Sheets API documentation](https://developers.google.com/sheets/api)\n",
    "- [`gspread` module](https://docs.gspread.org/en/v4.0.1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read data from a CSV file into a new dataframe.\n",
    "aug_csv_file = \"crime_data/tabula-NIBRS001-N_210708.csv\"\n",
    "\n",
    "crime_df = pd.read_csv(aug_csv_file, header=None)\n",
    "\n",
    "# Look at the first 40 rows in the dataframe.\n",
    "crime_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns with meaningful names.\n",
    "column_names = {0:\"Crime_Category\", \n",
    "                1:\"Crime_Type\", \n",
    "                2:\"NIBRS_Code\", \n",
    "                3:\"July\", \n",
    "                4:\"August\",\n",
    "                5:\"Monthly_Change\",\n",
    "                6:\"Percent_Change\",\n",
    "                7:\"Year_to_Date\"\n",
    "               }\n",
    "\n",
    "crime_df = crime_df.rename(columns=column_names)\n",
    "\n",
    "# Look at the first 30 rows in the dataframe.\n",
    "crime_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying what to change in columns and rows\n",
    "\n",
    "We'll begin tidying the data by identifying problematic patterns:\n",
    "\n",
    "- Need values for a single variable in each column.\n",
    "    - \"Crime_Category\" and \"Crime_Type\" also contain neighborhood information\n",
    "- Need to fill in missing values for a column where we know what should be there. \n",
    "    - \"Crime_Category\"\n",
    "- Don't need repeating headers within rows.\n",
    "- Don't need repeating calculations of totals within rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring values for a single variable in each column\n",
    "\n",
    "We need to pull neighborhood information into its own column, every row of data should have a neighborhood associated with it. Then we can make sure this information no longer occurs every once in a while as a row across the \"Crime_Category\" and \"Crime_Type\" columns.\n",
    "\n",
    "Once we've put those values into their own column, we can remove the rows that have neghborhood values in the wrong columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column called \"Neighborhood\" for neighborhoods that is empty.\n",
    "crime_df[\"Neighborhood\"] = np.nan\n",
    "\n",
    "# Look at the dataframe.\n",
    "crime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask to identify rows containing \"Neighborhood\" as a value in the \"Crime_Category\" column.\n",
    "mask = crime_df[\"Crime_Category\"] == \"Neighborhood\"\n",
    "\n",
    "# Look at the subset of rows in the dataframe where the boolean mask is true.\n",
    "crime_df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the new \"Neighborhood\" column with neighborhood names from the \"Crime_Type\" column.\n",
    "crime_df[\"Neighborhood\"] = crime_df[\"Crime_Type\"].where(mask, other=np.nan)\n",
    "\n",
    "# Look at the subset of rows in the dataframe where the boolean mask is true. Again.\n",
    "crime_df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the unique neighborhood names now in the \"Neighborhood\" column.\n",
    "crime_df[\"Neighborhood\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a slice of rows specified by the in index.\n",
    "crime_df.iloc[89:140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Downfill neighborhood values so that each row has a neighborhood name.\n",
    "crime_df[\"Neighborhood\"] = crime_df[\"Neighborhood\"].ffill()\n",
    "\n",
    "# Look at the dataframe.\n",
    "crime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting data using criteria\n",
    "We did this earlier with the \"Neighborhood\" column. To make other kinds of subsets based on criteria, we can apply a \"boolean mask\" and use logical operators:\n",
    "\n",
    "- Equals: ==\n",
    "- Not equals: !=\n",
    "- Greater than, less than: > or <\n",
    "- Greater than or equal to: >=\n",
    "- Less than or equal to: <="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now neighbohood information is in its own column!\n",
    "# Create a boolean mask to identify rows not containing \"Neighborhood\" as a value in the \"Crime_Category\" column.\n",
    "mask = crime_df[\"Crime_Category\"] != \"Neighborhood\"\n",
    "\n",
    "# Create a new dataframe excluding the rows that contain \"Neighborhood\" as a value in the \"Crime_Category\" column.\n",
    "crime_df = crime_df[mask].copy()\n",
    "\n",
    "crime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in missing values for a column (where we know what should be there)\n",
    "We just did this in the \"Neighbohood\" column, and we can do it again now that the \"Crime_Category\" column no longer includes \"Neighborhood\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that \"Crime_Category\" no longer includes neighborhood information, downfill missing values.\n",
    "crime_df[\"Crime_Category\"] = crime_df[\"Crime_Category\"].ffill()\n",
    "\n",
    "crime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and removing unneccessary rows \n",
    "\n",
    "- Don't need repeating headers within rows.\n",
    "- Don't need repeating calculations of totals within rows.\n",
    "\n",
    "**The pattern**: It looks like \"Crime_Type\" is always \"NaN\" when one of these is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a boolean mask to identify rows with null (\"NaN\") values in the \"Crime_Type\" column.\n",
    "mask = crime_df[\"Crime_Type\"].notnull()\n",
    "\n",
    "# Remove rows with null (\"NaN\") values in the \"Crime_Type\" column.\n",
    "crime_df = crime_df[mask].copy()\n",
    "\n",
    "crime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a closer look at the data after the first pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a list of the Crime Types.\n",
    "crime_df[\"Crime_Type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at a subset of the data, for a particular Crime Type.\n",
    "mask = crime_df[\"Crime_Type\"] == \"Theft From Building\"\n",
    "crime_df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated records\n",
    "\n",
    "It looks like there are rows that include exactly the same information repeated, these are duplicates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe dropping duplicates.\n",
    "dedup_crime_df = crime_df.copy()\n",
    "\n",
    "dedup_crime_df = dedup_crime_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take another look at the subset.\n",
    "mask = dedup_crime_df[\"Crime_Type\"] == \"Theft From Building\"\n",
    "\n",
    "dedup_crime_df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving to a file\n",
    "Whenever you're ready, you can write some data from a dataframe in Python to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe to a CSV file.\n",
    "dedup_crime_df[[\"Neighborhood\",\"Crime_Type\", \"Year_to_Date\"]].to_csv(\"total_crime_reports_august.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data within a Jupyter Notebook to make visual comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the values in the \"Year-to-Date\" column are treated as numbers.\n",
    "numeric = dedup_crime_df[\"Year_to_Date\"]\n",
    "\n",
    "dedup_crime_df[\"Year_to_Date\"] = pd.to_numeric(numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the mask with your choice of \"Crime_Type\".\n",
    "mask = dedup_crime_df[\"Crime_Type\"] == \"Theft From Building\"\n",
    "\n",
    "# Assign the subset to a new dataframe\n",
    "theft_df = dedup_crime_df[mask].copy()\n",
    "\n",
    "# Sort by \"Year_to_Date\" column and assign first 20 rows to a new dataframe.\n",
    "top_theft_df = theft_df.sort_values(by=[\"Year_to_Date\"], ascending=False).head(20)\n",
    "\n",
    "# Look at the dataframe.\n",
    "top_theft_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a bar chart to see the top neighborhoods reporting this \"Crime_Type\" and their year-to-date values.\n",
    "\n",
    "X = top_theft_df[\"Neighborhood\"]\n",
    "Y = top_theft_df[\"Year_to_Date\"]\n",
    "\n",
    "fig, ax = plt.subplots() \n",
    "chart = ax.barh(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting geospatial data to visualize neighborhoods\n",
    "\n",
    "I've downloaded the \"Neighborhoods and Wards Shapefile\" from the St. Louis Government data [website](https://www.stlouis-mo.gov/data/datasets/distribution.cfm?id=73)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a geodataframe from the neighborhood boundaries shape file\n",
    "shape_file = \"nbrhds_wards/Neighborhood_Boundaries.shp\"\n",
    "\n",
    "hood_gdf = gpd.read_file(shape_file)\n",
    "\n",
    "hood_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the number of rows and columns.\n",
    "hood_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at a plot of the geometries colored by neighborhood number.\n",
    "hood_gdf.plot(\"NHD_NUM\", legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back and take a look at the shape of the theft dataframe with all neighborhoods.\n",
    "theft_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging data from two dataframes\n",
    "\n",
    "You can merge (aka \"join\") two dataframes using the `pd.merge()` function in order to combine data into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes on the neighborhood name.\n",
    "joined_df = pd.merge(left=theft_df, right=hood_gdf, left_on=\"Neighborhood\", right_on=\"NHD_NAME\")\n",
    "\n",
    "# Check on the overlap in neighborhood name by looking at the size of the new dataframe.\n",
    "joined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the new joined dataframe.\n",
    "joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a geodataframe from the merged dataframes\n",
    "joined_gdf = gpd.GeoDataFrame(joined_df, geometry=\"geometry\")\n",
    "\n",
    "# Take a look at a plot of the geometries colored by numbers in the \"Year_to_Date\" column.\n",
    "joined_gdf.plot(\"Year_to_Date\", legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More cleaning awaits!\n",
    "\n",
    "We seem to have theft data for 89 neighborhoods, and shape files for 88. We then tried to join these two datasets together using the neighborhood names they each contained. Only 70 of the neighborhood names were an exact match. These could be typos, or minor differences in how the names were written in one dataset versus the other. And this sort of thing is very common.\n",
    "\n",
    "To clean the data further, we would need to identify which neighborhood names didn't match, and what our standard names for those neighborhoods should be in order to correct one or both of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theft_df[\"Neighborhood\"].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_gdf[\"NHD_NAME\"].sort_values().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Materials: Getting data into and out of a database with Python\n",
    "\n",
    "## Intro\n",
    "In some cases the data you want to access may be in a database, or you may want to put data into a database.\n",
    "\n",
    "Since I'm talking about tabular data, I'm going to focus on relational databases. These databases make sense when the amount of data you're working with gets large, and a system for managing the relationships between data in different tables matters.\n",
    "\n",
    "Here we'll assume we have access to an existing database. For reference, a \"simple\" database engine you can start with is SQLite, learn more about SQLite [here](https://www.sqlite.org/index.html).\n",
    "\n",
    "## Database adapters\n",
    "If you're already using Python to manipulate data, you can use an adapter to connect to a database within your code:\n",
    "\n",
    "- `sqlite3` for [SQLite databases](https://docs.python.org/3/library/sqlite3.html)\n",
    "- `psycopg2` for [PostgreSQL databases](https://www.psycopg.org/docs/usage.html)\n",
    "- `mysql-connector-python` for [MySQL databases](https://www.w3schools.com/python/python_mysql_getstarted.asp)\n",
    "\n",
    "## Overview\n",
    "The `sqlite3` module provides an interface for interacting with SQLite databases. It'll be my example for how using  database adapters works in Python.\n",
    "\n",
    "1. First, create a Connection object ito represent the database using `sqlite3.connect()`. \n",
    "2. Once you have a Connection, create a Cursor object with `.cursor()`.\n",
    "3. The Cursor can perform all kinds of SQL (Structured Query Language) commands with the `.execute()` method.\n",
    "4. Use `.commit()` to optionally save changes to the database.\n",
    "5. When done, close the connection with `.close()`.\n",
    "\n",
    "**Flow**: open connection -> create cursor -> execute SQL commands -> *commit changes ->* close connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the `sqlite3` module.\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to a database.\n",
    "# Note: SQLite databases are files, database engines with servers will require more parameters in order to connect.\n",
    "conn = sqlite3.connect('example.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cursor to execute commands.\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the cursor to work with the database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute an SQL query to create a new database table.\n",
    "sql_create_table = '''\n",
    "    CREATE TABLE tree\n",
    "    (id INT PRIMARY KEY NOT NULL,\n",
    "    name TEXT NOT NULL,\n",
    "    description TEXT,\n",
    "    rating REAL);\n",
    "    '''  \n",
    "\n",
    "c.execute(sql_create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute an SQL query to add data (one record) into a table.\n",
    "\n",
    "# The `sqlite3` module uses \"?\" as a placeholder wherever you want to use a value. \n",
    "# You then provide a tuple of values as the second argument to the cursor’s `execute()` method.\n",
    "# Note: Other database modules may use a different placeholder, for example `psycopg2` uses \"%s\".\n",
    "tree_record = (1, 'Sassafras', \"mitten-shaped and trilobed leaves\", 8)\n",
    "sql_insert = \"INSERT INTO tree (id, name, description, rating) VALUES (?,?,?,?);\"\n",
    "\n",
    "c.execute(sql_insert, tree_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute an SQL query to add data (multiple records) into a table.\n",
    "tree_records = [(2, 'American Hornbeam', 'muscular trunk', 7.75),\n",
    "                (3, 'Flowering Dogwood', 'stinky flowers', 6.50),\n",
    "                (4, 'Bald Cypress', 'knobby knees', 10),\n",
    "                (5, 'Lacebark Elm', 'flaky bark', 6.25),  \n",
    "            ]\n",
    "c.executemany('INSERT INTO tree VALUES (?,?,?,?)', tree_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commit changes to the database to make them persistent across sessions.\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Execute an SQL query to get data from an existing database table.\n",
    "c.execute(\"SELECT * FROM tree;\")\n",
    "\n",
    "# Fetch the results.\n",
    "c.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cursor.fetchall()` fetches all the rows of a query result. It returns all the rows as a list of tuples. An empty list is returned if there is no record to fetch.\n",
    "\n",
    "`cursor.fetchmany(size)` returns the number of rows specified by size argument. When called repeatedly this method fetches the next set of rows of a query result and returns a list of tuples. If no more rows are available, it returns an empty list.\n",
    "\n",
    "`cursor.fetchone()` method returns a single record or None if no more rows are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, treat the cursor as an \"iterator\".\n",
    "for row in c.execute(\"SELECT * FROM tree;\"):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `pandas` to work with a database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use `pandas` to read data from a database table directly into a dataframe.\n",
    "tree_df = pd.read_sql_query(\n",
    "    '''\n",
    "    SELECT * FROM tree;\n",
    "    ''',\n",
    "    conn)\n",
    "\n",
    "# Look at what's in the dataframe.\n",
    "tree_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data from a dataframe directly into a database table.\n",
    "schools_df.to_sql(\"schools\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use cursor as an \"iterator\" to see rows in the new database table.\n",
    "for row in c.execute(\n",
    "    '''\n",
    "    SELECT \n",
    "        SchoolName,\n",
    "        Grades,\n",
    "        Grade\n",
    "    FROM schools\n",
    "    WHERE Grades is \"9-12\" and Grade is \"A\";\n",
    "    '''\n",
    "):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing the database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the cursor and database connection.\n",
    "if(conn):\n",
    "    c.close()\n",
    "    conn.close()\n",
    "    print(\"The database connection is closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
